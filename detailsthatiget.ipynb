{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open-set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Set یا \"مجموعه باز\" در یادگیری ماشین و تشخیص الگوها به شرایطی اشاره دارد که مدل تنها با بخشی از تمام دسته‌های ممکن در طول آموزش مواجه است، اما در مرحله تست با داده‌هایی از دسته‌های ناشناخته برخورد می‌کند. به عبارت دیگر، در یک سیستم Open Set، مدل باید قادر باشد نه‌تنها داده‌هایی را که در دسته‌های آموزش‌دیده هستند، شناسایی کند، بلکه بتواند موارد ناشناخته یا غیرمربوط به کلاس‌های شناخته‌شده را نیز شناسایی کند.\n",
    "\n",
    "توضیح مفصل:\n",
    "در اکثر مسائل یادگیری ماشین سنتی (مانند طبقه‌بندی)، فرض بر این است که همه‌ی دسته‌ها یا کلاس‌ها که مدل ممکن است با آن‌ها مواجه شود، از پیش در داده‌های آموزشی مشخص شده‌اند. این وضعیت به عنوان Closed Set یا \"مجموعه بسته\" شناخته می‌شود، جایی که دسته‌ها در مرحله آموزش و تست یکی هستند. اما در دنیای واقعی، سیستم‌ها اغلب با داده‌هایی برخورد می‌کنند که به دسته‌هایی تعلق دارند که در طول آموزش دیده نشده‌اند. در چنین شرایطی، مدل باید بتواند به درستی تشخیص دهد که داده ورودی به دسته‌ای ناشناخته تعلق دارد."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # close-set\n",
    " کلاس‌هایی که مدل باید در مرحله تست یا پیش‌بینی با آن‌ها \n",
    " کار کند، از قبل در مرحله آموزش دیده شده‌اند. به بیان ساده‌تر، مدل یادگیری ماشین در مواجهه با داده‌های تست فقط با کلاس‌ها و برچسب‌هایی سر و کار دارد که از قبل در داده‌های آموزشی وجود داشته‌اند و هیچ کلاس ناشناخته‌ای در این مجموعه وجود ندارد.\n",
    "\n",
    "ویژگی‌های اصلی Close-Set:\n",
    "کلاس‌های مشخص و محدود: در یک مسئله مجموعه بسته، مدل تنها با تعدادی کلاس از پیش تعریف‌شده در طول آموزش و تست سروکار دارد. هیچ داده یا دسته جدیدی که مدل آن را ندیده باشد، وجود نخواهد داشت.\n",
    "\n",
    "آموزش و تست از کلاس‌های مشترک: در یک سناریوی Close-Set، داده‌های آموزشی و تست هر دو از همان کلاس‌ها تشکیل شده‌اند. برای مثال، اگر مدل بر روی دسته‌های \"گربه\"، \"سگ\"، و \"اسب\" آموزش دیده باشد، در مرحله تست نیز تنها داده‌هایی مربوط به همین دسته‌ها را خواهد دید.\n",
    "\n",
    "چالش کمتر نسبت به Open-Set: تشخیص در مجموعه بسته معمولاً ساده‌تر است، زیرا مدل فقط باید داده‌های دیده‌شده را طبقه‌بندی کند و نیازی به شناسایی داده‌های ناشناخته ندارد.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noisy detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "مسئله تشخیص برچسب‌های نویزی (Noisy Label Detection) به مشکلی در یادگیری ماشین اشاره دارد که در آن داده‌های آموزشی حاوی برچسب‌های اشتباه یا نادرست هستند. در یادگیری نظارت‌شده، مدل‌ها از داده‌های برچسب‌گذاری‌شده برای یادگیری الگوها استفاده می‌کنند، اما وقتی برچسب‌ها حاوی نویز باشند (یعنی برچسب اشتباه یا غلط به داده تخصیص داده شده باشد)، عملکرد مدل به شدت تحت تأثیر قرار می‌گیرد و دقت آن کاهش می‌یابد.\n",
    "\n",
    "منابع برچسب‌های نویزی:\n",
    "اشتباه انسانی: در فرآیند برچسب‌گذاری داده‌ها، ممکن است افرادی که داده‌ها را برچسب‌گذاری می‌کنند اشتباه کنند. این موضوع در مجموعه داده‌های بزرگ و پیچیده شایع است.\n",
    "خودکارسازی ناقص: گاهی از روش‌های خودکار برای برچسب‌گذاری داده‌ها استفاده می‌شود، که ممکن است خطاهایی در آن وجود داشته باشد.\n",
    "ابهام در داده‌ها: برخی داده‌ها ممکن است به راحتی قابل دسته‌بندی نباشند یا خود داده مبهم باشد، مانند تصاویر یا متون مبهم که نمی‌توان با اطمینان برچسب صحیحی به آن‌ها داد.\n",
    "اثرات برچسب‌های نویزی:\n",
    "کاهش دقت مدل: برچسب‌های نویزی باعث می‌شوند مدل الگوهای اشتباهی را یاد بگیرد و نتواند به درستی تعمیم دهد. در نهایت، این موضوع باعث کاهش دقت مدل در پیش‌بینی‌ها می‌شود.\n",
    "افزایش زمان و هزینه آموزش: زمانی که داده‌های نویزی به مدل داده می‌شود، زمان آموزش بیشتر می‌شود و مدل ممکن است به پارامترهای نامناسب تنظیم شود که نیاز به تصحیح دارد.\n",
    "عدم تعمیم‌پذیری مناسب: مدل‌هایی که با برچسب‌های نویزی آموزش می‌بینند، توانایی تعمیم‌دهی به داده‌های جدید و ناشناخته را از دست می‌دهند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention mechanism\n",
    "\n",
    "مکانیزم توجه (Attention Mechanism) یک تکنیک پیشرفته در یادگیری ماشین و به‌خصوص در مدل‌های عصبی است که به مدل کمک می‌کند تا بتواند روی بخش‌های مهم‌تری از ورودی تمرکز کند و اهمیت آن‌ها را درک نماید. این مکانیزم به‌طور گسترده در مسائلی مانند ترجمه ماشینی، پردازش زبان طبیعی (NLP)، و بینایی ماشین مورد استفاده قرار می‌گیرد.\n",
    "\n",
    "چرا مکانیزم توجه به وجود آمد؟\n",
    "در مسائل پیچیده‌ای مانند پردازش زبان طبیعی یا بینایی ماشین، هر ورودی دارای حجم زیادی از اطلاعات است که همگی دارای اهمیت یکسانی نیستند. برای مثال، در یک جمله، ممکن است برخی از کلمات مهم‌تر باشند و مدل نیاز داشته باشد تا توجه بیشتری به آن‌ها بکند. مکانیزم توجه به مدل اجازه می‌دهد تا به جای در نظر گرفتن تمام ورودی‌ها به صورت یکسان، روی قسمت‌هایی که برای وظیفه فعلی مهم‌تر هستند تمرکز کند و آن‌ها را اولویت‌بندی نماید.\n",
    "\n",
    "مفهوم کلی مکانیزم توجه\n",
    "مکانیزم توجه به هر قسمت از داده‌های ورودی، یک وزن اختصاص می‌دهد که این وزن نشان‌دهنده میزان اهمیت آن قسمت است. سپس مدل بیشتر روی قسمت‌هایی که وزن بالاتری دارند تمرکز می‌کند و از آن‌ها برای تصمیم‌گیری استفاده می‌کند."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Map Consistency** به معنای **یکسانی یا سازگاری نقشه توجه** در زمینه‌های مختلف یادگیری ماشین، به ویژه در مدل‌های مبتنی بر مکانیزم توجه (Attention Mechanism\n",
    ") مانند شبکه‌های عصبی بازگشتی (RNN)، شبکه‌های عصبی کانولوشنی (CNN)، و ترانسفورمرها (Transformers) است.\n",
    "\n",
    "مکانیزم توجه به مدل‌ها کمک می‌کند تا بتوانند روی قسمت‌های مهم داده‌های ورودی تمرکز کنند و ارتباطات یا ویژگی‌های کلیدی را بهتر درک کنند. **نقشه توجه (Attention Map)** نشان می‌دهد که مدل در هر گام از پردازش، چقدر توجه خود را به بخش‌های مختلف داده‌ها معطوف کرده است.\n",
    "\n",
    "### تعریف Attention Map Consistency:\n",
    "**Attention Map Consistency** به این موضوع اشاره دارد که مدل در طی پردازش ورودی‌های مشابه یا تکراری، بتواند الگوی توجه یکسان یا سازگاری را حفظ کند. این سازگاری به معنای آن است که مدل در پردازش‌های مختلف روی همان داده، همان بخش‌های مهم یا ویژگی‌های کلیدی را شناسایی کرده و به آن‌ها توجه می‌کند.\n",
    "\n",
    "### اهمیت Attention Map Consistency:\n",
    "- **پایداری و اعتماد به مدل**: اگر یک مدل یادگیری ماشین بتواند نقشه توجه پایدار و سازگاری را برای ورودی‌های مشابه ارائه دهد، نشان‌دهنده این است که مدل به درستی یاد گرفته است کدام قسمت‌های داده مهم هستند. این موضوع برای کاربردهایی مانند بینایی ماشین و پردازش زبان طبیعی که نیاز به تمرکز بر جزئیات خاص دارند، اهمیت زیادی دارد.\n",
    "  \n",
    "- **تعمیم‌پذیری بهتر**: سازگاری در نقشه توجه می‌تواند نشان‌دهنده این باشد که مدل توانایی تعمیم‌پذیری به داده‌های مشابه یا داده‌های جدید را دارد و می‌تواند در شرایط مختلف عملکرد ثابتی از خود نشان دهد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cycle training\n",
    "\n",
    "Cycle Training یا آموزش چرخه‌ای یک روش یا تکنیک در آموزش مدل‌های یادگیری ماشین است که طی آن، داده‌های آموزشی به صورت دوره‌ای یا چرخه‌ای به مدل ارائه می‌شوند تا مدل بتواند با به‌روز‌رسانی‌های پیوسته و تکراری، بهبود یابد. این روش به‌ویژه در یادگیری نیمه‌نظارتی (Semi-supervised Learning)، یادگیری بدون نظارت (Unsupervised Learning)، و در برخی معماری‌های خاص مانند شبکه‌های مولد تقابلی (GANs) استفاده می‌شود.\n",
    "اصول Cycle Training:\n",
    "آموزش چرخه‌ای شامل تکرار مداوم یک فرآیند آموزشی است که می‌تواند شامل تغییرات در داده‌ها یا مدل باشد. هدف این است که با هر چرخه جدید، مدل بهبود یابد و توانایی خود را برای تعمیم به داده‌های جدید افزایش دهد.\n",
    "کاربردهای Cycle Training:\n",
    "1.\tCycleGAN: یکی از معروف‌ترین کاربردهای آموزش چرخه‌ای در CycleGAN است، یک نوع شبکه مولد تقابلی (Generative Adversarial Network) که برای تبدیل تصویر به تصویر بدون استفاده از برچسب‌ها استفاده می‌شود. در CycleGAN، چرخه‌های متعددی از آموزش وجود دارد که به مدل کمک می‌کند تا بتواند یاد بگیرد چگونه تصاویر را از یک دامنه به دامنه دیگر تبدیل کند و در عین حال ثبات و سازگاری آن‌ها را حفظ کند. به عنوان مثال، تبدیل تصویر از دامنه «اسب» به «گورخر» و بالعکس.\n",
    "مکانیزم CycleGAN دو مولد (Generator) و دو متمایزکننده (Discriminator) دارد که در چرخه‌های جداگانه آموزش می‌بینند:\n",
    "o\tمولد اول وظیفه تبدیل یک تصویر از دامنه A به دامنه B را دارد.\n",
    "o\tمولد دوم تصویر تولید شده را دوباره به دامنه A بازمی‌گرداند.\n",
    "o\tمتمایزکننده‌ها بررسی می‌کنند که آیا تصاویر تولیدشده واقعی هستند یا نه، و به بهبود عملکرد مولدها کمک می‌کنند. این چرخه‌ها به مدل اجازه می‌دهند که یاد بگیرد چگونه دو دامنه کاملاً متفاوت را به یکدیگر تبدیل کند، در حالی که ویژگی‌های اصلی هر دامنه حفظ می‌شوند.\n",
    "2.\tآموزش با داده‌های مصنوعی: در برخی موارد، از آموزش چرخه‌ای برای ترکیب داده‌های مصنوعی و واقعی استفاده می‌شود. به این صورت که در هر چرخه جدید، داده‌های مصنوعی بیشتری تولید و به مدل اضافه می‌شود تا مدل به صورت تدریجی و مداوم بهتر شود.\n",
    "3.\tیادگیری چند وظیفه‌ای (Multi-task Learning): در یادگیری چند وظیفه‌ای، مدل ممکن است در چرخه‌های مختلف برای وظایف مختلف آموزش ببیند. این چرخه‌ها به مدل کمک می‌کنند تا از داده‌های مربوط به هر وظیفه یاد بگیرد و در عین حال ارتباط بین وظایف مختلف را حفظ کند.\n",
    "مزایای Cycle Training:\n",
    "1.\tبهبود تدریجی مدل: با هر چرخه جدید، مدل از داده‌ها و خطاهای چرخه قبلی یاد می‌گیرد و عملکرد خود را بهبود می‌دهد.\n",
    "2.\tحفظ تعادل بین دامنه‌ها: در مدل‌های مانند CycleGAN، چرخه‌های آموزشی کمک می‌کنند که مدل تعادل بین دو دامنه مختلف (مانند تبدیل تصویر از یک سبک به سبک دیگر) را حفظ کند.\n",
    "3.\tاستفاده از داده‌های بدون برچسب: در برخی از روش‌های نیمه‌نظارتی یا بدون نظارت، آموزش چرخه‌ای به مدل اجازه می‌دهد که از داده‌های بدون برچسب یا داده‌های تولید شده توسط خود مدل، به طور موثر یاد بگیرد.\n",
    "محدودیت‌ها:\n",
    "•\tپیچیدگی محاسباتی: آموزش چرخه‌ای می‌تواند از لحاظ محاسباتی پیچیده و زمان‌بر باشد، به‌ویژه در مدل‌های بزرگی مانند CycleGAN که نیاز به چندین مولد و متمایزکننده دارد.\n",
    "•\tتنظیم پارامترها: برای دستیابی به عملکرد بهینه، نیاز به تنظیم دقیق پارامترها و کنترل کردن روند چرخه‌های آموزشی وجود دارد.\n",
    "نتیجه‌گیری:\n",
    "Cycle Training یا آموزش چرخه‌ای یک تکنیک مفید در یادگیری ماشین است که به مدل‌ها اجازه می‌دهد از تکرارهای مداوم و بازخوردهای چرخه‌ای برای بهبود عملکرد خود استفاده کنند. این روش به‌ویژه در مسائل بدون نظارت و نیمه‌نظارتی مانند CycleGAN کاربرد دارد و می‌تواند منجر به تعمیم‌پذیری بهتر مدل در شرایط مختلف شود.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# openmax\n",
    "\n",
    "Open-Set Recognition (OSR) یک مسئله مهم در یادگیری ماشین \n",
    "است که در آن مدل نه‌تنها باید داده‌های آموزش‌دیده را شناسایی کند، بلکه باید بتواند داده‌های ناشناخته (که در طی فرآیند آموزش دیده نشده‌اند) را تشخیص دهد و آن‌ها را به‌عنوان کلاس‌های ناشناخته دسته‌بندی کند. این چالش زمانی به وجود می‌آید که داده‌های تست شامل کلاس‌هایی باشد که در داده‌های آموزشی وجود نداشته است.\n",
    "در یک سیستم Open-Set Recognition، هدف این است که مدل قادر باشد داده‌هایی را که به کلاس‌های \"آشنا\" تعلق دارند، درست طبقه‌بندی کند و داده‌هایی را که به کلاس‌های \"ناشناخته\" تعلق دارند، به‌عنوان ناشناخته تشخیص دهد.\n",
    "روش‌های Open-Set Recognition\n",
    "در ادامه، دو روش پرکاربرد برای Open-Set Recognition توضیح داده شده است:\n",
    "1. OpenMax\n",
    "یکی از روش‌های مطرح برای OSR OpenMax است که به‌عنوان بهبود‌یافته‌ای از روش‌های طبقه‌بندی مبتنی بر شبکه‌های عصبی (مانند Softmax) توسعه داده شده است.\n",
    "نحوه عملکرد OpenMax:\n",
    "•\tاستفاده از Softmax اصلاح‌شده: در یک مدل شبکه عصبی معمول، Softmax آخرین لایه خروجی است که برای محاسبه احتمال کلاس‌ها استفاده می‌شود. Softmax معمولاً داده‌های ناشناخته را به یکی از کلاس‌های آموزش‌دیده دسته‌بندی می‌کند، زیرا همیشه یک احتمال برای هر کلاس تولید می‌کند.\n",
    "•\tمدل‌سازی عدم قطعیت (Uncertainty): در OpenMax، به جای استفاده مستقیم از Softmax، از توزیع آماری روی بردارهای ویژگی‌ها (Feature Vectors) برای تخمین میزان عدم قطعیت استفاده می‌شود. این فرآیند به مدل کمک می‌کند تا تشخیص دهد که آیا ورودی به یکی از کلاس‌های شناخته‌شده تعلق دارد یا نه.\n",
    "•\tقابلیت تشخیص ناشناخته‌ها: اگر مدل تشخیص دهد که یک داده به هیچ‌کدام از کلاس‌های شناخته‌شده تعلق ندارد، آن را به‌عنوان داده ناشناخته (Open Set) طبقه‌بندی می‌کند.\n",
    "مراحل اصلی در OpenMax:\n",
    "1.\tمحاسبه Activation Vectors (AVs): ابتدا مدل، خروجی‌های یک لایه‌ی مخفی از شبکه عصبی را به عنوان بردار فعال‌سازی (AVs) محاسبه می‌کند.\n",
    "2.\tتخمین Weibull Distributions: توزیع Weibull بر روی این بردارهای فعال‌سازی (AVs) اعمال می‌شود تا آستانه‌ای برای شناسایی نمونه‌های ناشناخته تنظیم شود.\n",
    "3.\tترکیب با Softmax: نتایج حاصل از Weibull با خروجی‌های Softmax ترکیب شده و تصمیم‌گیری نهایی در مورد ناشناخته یا شناخته بودن داده انجام می‌شود.\n",
    "2. DIAS (Deep Investigation of Activation Spaces)\n",
    "DIAS یکی دیگر از روش‌های OSR است که با استفاده از فضای ویژگی‌های عمیق شبکه‌های عصبی (Deep Activation Spaces) داده‌های ناشناخته را تشخیص می‌دهد. این روش بر اساس بررسی دقیق فضاهای فعال‌سازی شبکه‌های عصبی به‌دنبال کشف الگوهایی است که به داده‌های ناشناخته مربوط هستند.\n",
    "نحوه عملکرد DIAS:\n",
    "•\tتحلیل بردارهای فعال‌سازی (Activation Vectors): مشابه OpenMax، DIAS نیز بر روی فضای ویژگی‌های عمیق (Deep Activation Space) تمرکز دارد. اما به‌جای استفاده از Softmax یا توزیع‌های آماری، از رویکردهای تحلیلی عمیق‌تر برای تمایز بین کلاس‌های شناخته‌شده و ناشناخته استفاده می‌کند.\n",
    "•\tتحلیل هندسی فضا: DIAS از تکنیک‌های تحلیل هندسی برای بررسی توزیع نمونه‌های مختلف در فضای ویژگی استفاده می‌کند. این روش کمک می‌کند تا داده‌هایی که در فضای فعال‌سازی با الگوهای جدید و ناشناخته‌ای ظاهر می‌شوند، شناسایی گردند.\n",
    "•\tکاهش بعد و تحلیل فضاهای فعال‌سازی: این روش می‌تواند با استفاده از تکنیک‌های کاهش ابعاد مانند PCA یا t-SNE به درک بهتر فضای ویژگی کمک کند و به‌طور موثرتری داده‌های ناشناخته را از داده‌های شناخته‌شده جدا کند.\n",
    "تفاوت‌های OpenMax و DIAS:\n",
    "•\tOpenMax بیشتر بر روی ترکیب احتمالی مبتنی بر Softmax و Weibull برای شناسایی ناشناخته‌ها تمرکز دارد، در حالی که DIAS از تحلیل هندسی عمیق‌تری در فضای ویژگی‌ها استفاده می‌کند.\n",
    "•\tOpenMax برای شناسایی ناشناخته‌ها از توزیع‌های آماری استفاده می‌کند، در حالی که DIAS به تحلیل عمیق‌تر و استفاده از کاهش بعد برای شناسایی داده‌های ناشناخته می‌پردازد.\n",
    "سایر روش‌های Open-Set Recognition:\n",
    "1.\tSupport Vector Machines (SVMs): برخی از روش‌های Open-Set Recognition از SVM به عنوان پایه استفاده می‌کنند. به‌ویژه، SVM با مرز تصمیم‌گیری باز (Open Boundary Decision SVM) که از مرزهای تصمیم‌گیری برای جدا کردن کلاس‌های ناشناخته از کلاس‌های شناخته‌شده استفاده می‌کند.\n",
    "2.\tExtreme Value Theory (EVT): بسیاری از روش‌های OSR، مانند OpenMax، از نظریه Extreme Value Theory (EVT) برای مدل‌سازی ناهنجاری‌ها و داده‌های ناشناخته استفاده می‌کنند. این نظریه به تحلیل داده‌هایی که بسیار نادر هستند یا خارج از محدوده توزیع کلی داده‌ها قرار دارند، کمک می‌کند.\n",
    "3.\tAutoencoders: خودرمزگذارها (Autoencoders) نیز می‌توانند در Open-Set Recognition استفاده شوند. این مدل‌ها به این صورت عمل می‌کنند که داده‌های شناخته‌شده را به‌خوبی بازسازی می‌کنند و داده‌های ناشناخته را با خطای بازسازی بالا شناسایی می‌کنند.\n",
    "\n",
    "\n",
    "\n",
    "# مدل‌های تفکیکی (Discriminative Models)\n",
    "\n",
    "مدل‌های تفکیک‌کننده دسته‌ای از مدل‌ها هستند که هدفشان تمایز بین کلاس‌های شناخته‌شده و ناشناخته است. این مدل‌ها به‌طور مستقیم تصمیم‌گیری می‌کنند که یک نمونه ورودی به کدام کلاس تعلق دارد. به عنوان مثال، در زمینه تشخیص حالت باز، مدل‌های تفکیک‌کننده مانند OpenMax از استراتژی‌هایی استفاده می‌کنند که به جای استفاده از لایه نرم‌ماکس (Softmax) معمول، از روش‌های کالیبراسیون توزیع دیگری استفاده می‌کنند تا احتمال‌های صحیح‌تری برای نمونه‌های ناشناخته تخصیص داده شود.\n",
    "\n",
    "در این روش‌ها، مدل‌ها تلاش می‌کنند تفاوت بین کلاس‌های شناخته‌شده و داده‌هایی که به این کلاس‌ها تعلق ندارند را به‌خوبی تشخیص دهند، اما مشکل اصلی این مدل‌ها این است که زمانی که فاصله بین کلاس‌ها کوچک است (مانند مجموعه‌داده‌های FER که تفاوت‌های چهره‌ای بسیار ظریف هستند)، عملکرد آنها کاهش پیدا می‌کند و ممکن است نمونه‌های ناشناخته را به اشتباه به کلاس‌های شناخته‌شده تخصیص دهند.\n",
    "\n",
    "\n",
    "# مدل‌های تولیدی (Generative Models)\n",
    "\n",
    "معنی: مدل‌هایی که توزیع داده‌های شناخته‌شده را یاد می‌گیرند و می‌توانند نقاط داده جدیدی را از آن توزیع تولید کنند.\n",
    "مفهوم: این مدل‌ها توزیع کلاس‌های نادیده را با تولید نمونه‌های مصنوعی که ویژگی‌های آن کلاس‌ها را نزدیک می‌کنند، پیش‌بینی می‌کنند. تکنیک‌هایی مانند شبکه‌های رقابتی مولد (GANs) در این دسته قرار می‌گیرند.\n",
    "دسته‌بندها (Classifiers)\n",
    "\n",
    "معنی: الگوریتم‌ها یا مدل‌هایی که برای طبقه‌بندی داده‌های ورودی به کلاس‌های خاص استفاده می‌شوند.\n",
    "مفهوم: دسته‌بندها در یادگیری ماشین برای پیش‌بینی برچسب‌ها بر اساس ویژگی‌های ورودی بسیار ضروری هستند. آن‌ها الگوها را از داده‌های آموزشی یاد می‌گیرند و این الگوها را به داده‌های جدید و دیده‌نشده اعمال می‌کنند.\n",
    "\n",
    "\n",
    " # لایه softmax (Softmax Layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # شبکه‌های رقابتی مولد  (Generative Adversarial Networks - GANs)\n",
    "\n",
    "معنی: یک دسته از چارچوب‌های یادگیری ماشین که در آن دو شبکه عصبی در برابر یکدیگر رقابت می‌کنند.\n",
    "مفهوم: GANها شامل یک تولیدکننده هستند که داده‌های جعلی ایجاد می‌کند و یک تمایزدهنده که سعی می‌کند بین داده‌های واقعی و تولید شده تمایز قائل شود. این فرایند رقابتی به تولیدکننده کمک می‌کند تا با گذشت زمان بهبود یابد و می‌تواند برای تولید نمونه‌هایی از کلاس‌های نادیده استفاده شود.\n",
    "\n",
    "\n",
    " # فاصله بین کلاس‌ها (Inter-Class Distance)\n",
    "\n",
    "معنی: اندازه‌گیری فاصله بین کلاس‌های مختلف در فضای ویژگی.\n",
    "مفهوم: در داده‌هایی که فاصله‌های بین کلاس‌ها بزرگ است، کلاس‌ها به خوبی از هم جدا شده‌اند و تمایز بین آن‌ها برای مدل‌ها آسان‌تر است. در مقابل، داده‌های FER معمولاً فاصله‌های بین کلاس‌ها کوچکتری دارند، که شناسایی تفاوت‌های بین احساسات چهره را دشوار می‌کند.\n",
    "\n",
    " # دسته‌بندها (Classifiers)\n",
    "\n",
    "معنی: الگوریتم‌ها یا مدل‌هایی که برای طبقه‌بندی داده‌های ورودی به کلاس‌های خاص استفاده می‌شوند.\n",
    "مفهوم: دسته‌بندها در یادگیری ماشین برای پیش‌بینی برچسب‌ها بر اساس ویژگی‌های ورودی بسیار ضروری هستند. آن‌ها الگوها را از داده‌های آموزشی یاد می‌گیرند و این الگوها را به داده‌های جدید و دیده‌نشده اعمال می‌کنند.\n",
    "\n",
    " # لایه softmax (Softmax Layer)\n",
    "\n",
    "معنی: لایه‌ای که در شبکه‌های عصبی برای تبدیل نمرات خروجی خام به احتمال‌ها استفاده می‌شود.\n",
    "مفهوم: تابع softmax خروجی شبکه عصبی را به یک توزیع احتمال نرمال می‌کند و این امکان را برای طبقه‌بندی چندکلاسه فراهم می‌کند. در OSR، ممکن است این لایه با روش‌های جایگزین تعویض شود تا بهتر با کلاس‌های ناشناخته مدیریت شود.\n",
    "شبکه‌های رقابتی مولد (Generative Adversarial Networks - GANs)\n",
    "\n",
    "معنی: یک دسته از چارچوب‌های یادگیری ماشین که در آن دو شبکه عصبی در برابر یکدیگر رقابت می‌کنند.\n",
    "مفهوم: GANها شامل یک تولیدکننده هستند که داده‌های جعلی ایجاد می‌کند و یک تمایزدهنده که سعی می‌کند بین داده‌های واقعی و تولید شده تمایز قائل شود. این فرایند رقابتی به تولیدکننده کمک می‌کند تا با گذشت زمان بهبود یابد و می‌تواند برای تولید نمونه‌هایی از کلاس‌های نادیده استفاده شود.\n",
    "فاصله بین کلاس‌ها (Inter-Class Distance)\n",
    "\n",
    "معنی: اندازه‌گیری فاصله بین کلاس‌های مختلف در فضای ویژگی.\n",
    "مفهوم: در داده‌هایی که فاصله‌های بین کلاس‌ها بزرگ است، کلاس‌ها به خوبی از هم جدا شده‌اند و تمایز بین آن‌ها برای مدل‌ها آسان‌تر است. در مقابل، داده‌های FER معمولاً فاصله‌های بین کلاس‌ها کوچکتری دارند، که شناسایی تفاوت‌های بین احساسات چهره را دشوار می‌کند\n",
    "\n",
    "# Overconfidence\n",
    "\n",
    "معنی: تمایل مدل‌ها برای دادن پیش‌بینی‌های قوی حتی در مواقعی که داده‌ها نادیده هستند.\n",
    "مفهوم: در یادگیری عمیق، این پدیده به این معنی است که وقتی مدل‌ها با داده‌های جدید و ناشناخته مواجه می‌شوند، معمولاً آنها را به یکی از کلاس‌های شناخته‌شده نسبت می‌دهند، که می‌تواند منجر به اشتباهات جدی شود.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "------------------------------------------------------\n",
    "------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "این مقاله به یکی از مشکلات اساسی در حوزه تشخیص احساسات چهره، یعنی مواجهه با دسته‌های باز (Open-Set) از احساسات، و راهکار پیشنهادی نویسندگان برای رفع این چالش اشاره دارد.\n",
    "\n",
    "مسئله: مدل‌های سنتی تشخیص احساسات چهره (FER) معمولاً بر روی مجموعه داده‌هایی آموزش داده می‌شوند که شامل هفت دسته‌بندی اساسی از احساسات چهره هستند. اما در کاربردهای دنیای واقعی، احساسات بسیاری وجود دارند که در این دسته‌بندی‌ها قرار نمی‌گیرند، از جمله احساسات ترکیبی و ناشناخته. این مسئله باعث می‌شود که مدل‌های FER در شناسایی احساسات جدید دچار خطا شوند و نتوانند به درستی با احساسات چهره ناشناخته تطابق پیدا کنند.\n",
    "\n",
    "راه‌حل پیشنهادی: مقاله به معرفی وظیفه جدیدی به نام \"تشخیص احساسات چهره در فضای باز\" (open-set FER) می‌پردازد. در این روش، نویسندگان چالش تشخیص احساسات ناشناخته را به یک مسئله تشخیص برچسب‌های نویزی تبدیل می‌کنند. به دلیل فاصله کم بین دسته‌های مختلف احساسات چهره، تمایز میان احساسات شناخته‌شده و ناشناخته دشوار می‌شود. برای رفع این مشکل، یک روش مبتنی بر سازگاری نقشه توجه (Attention Map Consistency) و آموزش چرخه‌ای (Cycle Training) پیشنهاد شده است. این روش از فاصله کم بین دسته‌های مختلف احساسات به عنوان یک مزیت استفاده می‌کند و به شناسایی بهتر نمونه‌های دسته باز کمک می‌کند.\n",
    "\n",
    "نتایج: آزمایشات نشان می‌دهد که روش پیشنهادی به طور قابل توجهی بهتر از روش‌های موجود مانند DIAS و OpenMax عمل می‌کند و در تشخیص احساسات ناشناخته عملکرد بهتری دارد.\n",
    "\n",
    "ABSTRACT:\n",
    "مینه: مدل‌های تشخیص احساسات چهره (FER) نقش مهمی در تعاملات انسان و ماشین ایفا می‌کنند و به دستگاه‌ها کمک می‌کنند تا احساسات انسان را بهتر درک کنند. با این حال، این مدل‌ها با چالش‌های بزرگی مواجه هستند، چرا که اغلب در مواجهه با احساسات چهره ناشناخته که در دسته‌بندی‌های از پیش تعریف‌شده نمی‌گنجند، ناتوان هستند.\n",
    "\n",
    "چالش‌ها: مدل‌های فعلی اغلب احساسات جدید را به‌اشتباه به یکی از دسته‌های شناخته‌شده نسبت می‌دهند. یکی از دلایل اصلی این مسئله این است که مدل‌های یادگیری عمیق به‌طور معمول در مواجهه با داده‌های ناشناخته، سطح بالایی از اطمینان دارند، حتی اگر داده‌ها قبلاً دیده نشده باشند. روش‌های فعلی تشخیص دسته باز، مانند DIAS و OpenMax، در FER به خوبی عمل نمی‌کنند زیرا فاصله بین دسته‌های مختلف احساسات چهره در انسان‌ها بسیار کم است و این امر باعث می‌شود که احساسات ناشناخته بسیار شبیه به احساسات شناخته‌شده به نظر برسند.\n",
    "\n",
    "مشارکت: نویسندگان این مقاله پیشنهاد می‌کنند که این مشکل را می‌توان به یک مزیت تبدیل کرد و با در نظر گرفتن این چالش به عنوان یک مسئله تشخیص برچسب‌های نویزی، می‌توان تشخیص بهتری از احساسات دسته باز ارائه داد. روش ارائه‌شده نه تنها به بهبود دقت تشخیص کمک می‌کند، بلکه قدرت تمایز بیشتری در مواجهه با احساسات ناشناخته دارد.\n",
    "\n",
    "introduction :\n",
    "\n",
    "**پیش‌زمینه:** مدل‌های تشخیص احساسات چهره (FER) برای تعامل انسان و کامپیوتر بسیار حیاتی هستند و به ماشین‌ها کمک می‌کنند تا احساسات انسانی را تفسیر کنند. با این حال، مدل‌های فعلی در مواجهه با حالات ناشناخته چهره که در دسته‌های از پیش تعریف شده قرار نمی‌گیرند، مشکل دارند.\n",
    "\n",
    "**چالش‌ها:** این مدل‌ها اغلب حالات جدید را به اشتباه به عنوان حالات موجود دسته‌بندی می‌کنند، زیرا مدل‌های یادگیری عمیق در مواجهه با داده‌های دیده نشده، سطوح بالایی از اعتماد به نفس نشان می‌دهند. روش‌های فعلی تشخیص دسته‌باز (مانند DIAS و OpenMax) عملکرد مناسبی در تشخیص احساسات چهره ندارند، زیرا چهره‌های انسانی دارای فاصله کم بین دسته‌های مختلف هستند که باعث می‌شود حالات ناشناخته بسیار شبیه به حالات شناخته‌شده به نظر برسند.\n",
    "\n",
    "**مشارکت:** نویسندگان پیشنهاد می‌کنند که مشکل فاصله کم بین دسته‌ها را به یک مزیت تبدیل کنند و این چالش را به عنوان یک مسئله برچسب نویزی در نظر بگیرند، که این رویکرد به تشخیص بهتر حالات ناشناخته در دسته‌باز کمک می‌کند.\n",
    "\n",
    "\n",
    "RELATED WORK - FACIAL EXPRESSION RECOGNITION :( overview of prior studies and techniques in the field of Facial Expression Recognition (FER).)\n",
    "\n",
    "\n",
    "روش‌های قبلی:\n",
    "روش‌های مختلفی برای FER پیشنهاد شده است، از جمله:\n",
    "\n",
    "لی، دنگ و دو (۲۰۱۷): از جمع‌سپاری برای شبیه‌سازی شناسایی احساسات انسانی استفاده کردند و روش‌هایی را معرفی کردند که سعی دارند نحوه شناسایی احساسات توسط انسان‌ها را منعکس کنند.\n",
    "فرزانه و کی (۲۰۲۱): واریانت جدیدی از مرکزیت را پیشنهاد کردند تا هم شباهت درون‌کلاسی (اطمینان از اینکه تصاویر از همان کلاس نزدیک به هم هستند) و هم جداسازی بین‌کلاسی (نگه‌داشتن تصاویر کلاس‌های مختلف دور از هم) را به حداکثر برسانند.\n",
    "ژانگ، وانگ و دنگ (۲۰۲۱): یادگیری مقایسه‌ای نسبی را معرفی کردند که بر آموزش مدل‌های FER از طریق مقایسه‌ی شباهت یا تفاوت حالات چهره تمرکز دارد.\n",
    "رُوان و همکاران (۲۰۲۱): روش‌هایی برای استخراج ویژگی‌های مرتبط با احساسات از طریق تجزیه طراحی کردند و بر مهم‌ترین جنبه‌های حالات چهره تمرکز کردند.\n",
    "\n",
    "محدودیت‌های روش‌های موجود:\n",
    "این پاراگراف به این نکته اشاره دارد که در حالی که این روش‌ها در FER بسته (جایی که تعداد کلاس‌ها ثابت است) مؤثر هستند، اما برای مدیریت FER باز ناکافی هستند. بیشتر روش‌های FER موجود پیش‌بینی‌های مطمئن برای داده‌های باز (احساسات ناشناخته) انجام می‌دهند که محدودیت‌هایی در کاربردهای دنیای واقعی ایجاد می‌کند، زیرا نمی‌توانند احساساتی را که بخشی از مجموعه داده‌های آموزشی نبوده‌اند، به درستی شناسایی کنند.\n",
    "\n",
    "\n",
    "# open set recognition\n",
    "\n",
    "\n",
    "شناسایی باز (Open-Set Recognition)\n",
    "این بخش به چالش شناسایی باز (OSR) می‌پردازد، که زمانی اتفاق می‌افتد که مدل‌ها با کلاس‌هایی مواجه می‌شوند که در طول آموزش مشاهده نشده‌اند. دو جریان اصلی از روش‌های OSR وجود دارد:\n",
    "\n",
    "مدل‌های تفکیکی: این روش‌ها از دسته‌بندها برای تشخیص بین کلاس‌های شناخته‌شده و ناشناخته استفاده می‌کنند (به‌عنوان مثال، OpenMax). این مدل‌ها از استراتژی‌هایی مانند جایگزینی لایه softmax با روش‌های کالیبراسیون توزیع دیگر استفاده می‌کنند.\n",
    "\n",
    "مدل‌های تولیدی: این مدل‌ها توزیع کلاس‌های نادیده را با تولید داده‌هایی که به آن‌ها نزدیک است، پیش‌بینی می‌کنند. این رویکردها به تکنیک‌هایی مانند شبکه‌های رقابتی مولد (GAN) وابسته هستند.\n",
    "\n",
    "با این حال، هر دو جریان در datasets که فاصله‌های بین کلاس‌ها بزرگ است (مانند CIFAR-10) بهتر عمل می‌کنند، اما زمانی که بر روی داده‌های FER که در آن‌ها احساسات چهره ظریف است و تفاوت‌های بین کلاس‌ها کوچکتر است، اعمال شوند، شکست می‌خورند. این بخش تأکید می‌کند که روش‌های موجود OSR برای FER مناسب نیستند\n",
    "\n",
    "# problem definition \n",
    "\n",
    "تعریف مشکل\n",
    "\n",
    "نویسندگان مشکل شناسایی احساسات چهره در حالت باز (Open-Set FER) را به صورت زیر تعریف می‌کنند:\n",
    "\n",
    "شناسایی احساسات چهره در حالت بسته (Closed-set FER): بر روی داده‌هایی با تعداد مشخصی از کلاس‌ها (مثلاً هفت احساس اصلی) آموزش می‌بیند.\n",
    "شناسایی احساسات چهره در حالت باز (Open-set FER): با ابرازهای جدید و قبلاً دیده‌نشده در یک محیط واقعی مواجه می‌شود. چالش این است که مشخص شود آیا یک نمونه خاص به یک کلاس شناخته‌شده تعلق دارد یا از یک کلاس نادیده (کلاس K+1، کلاس جدید) است.\n",
    "مشکل شناسایی احساسات چهره این است که نمونه‌های حالت باز (open-set) بسیار مشابه نمونه‌های حالت بسته (closed-set) به نظر می‌رسند به دلیل فاصله کوچک بین کلاس‌ها، که منجر به اعتماد به نفس بیش از حد در مدل‌های یادگیری عمیق می‌شود که ابرازهای دیده‌نشده را به یکی از کلاس‌های شناخته‌شده طبقه‌بندی می‌کنند. یک روش جدید برای مقابله با این چالش ضروری است.\n",
    "\n",
    "\n",
    "# METHOD \n",
    "\n",
    "### pseudo lables:\n",
    "\n",
    "یک مدل FER در حالت بسته آموزش داده می‌شود و برای تمامی نمونه‌ها برچسب‌های شبه تولید می‌کند. نمونه‌های حالت باز، برچسب‌های نادرست به تمام کلاس‌های شناخته‌شده اختصاص می‌گیرند که در واقع برچسب‌های \"نویزی\" ایجاد می‌کند.\n",
    "برخلاف مجموعه‌داده‌های سنتی، در FER برچسب‌های نادرست بین چندین کلاس شناخته‌شده توزیع می‌شوند، به جای اینکه به یک کلاس معنایی مشابه متمرکز شوند.\n",
    "\n",
    "\n",
    "# cycle trainig :\n",
    "\n",
    "دو مدل به صورت چرخه‌ای آموزش داده می‌شوند. یک مدل با برچسب‌های شبه آموزش می‌بیند و نمونه‌های پاک را برای آموزش مدل دیگر انتخاب می‌کند. این مدل‌ها به طور متناوب یکدیگر را آموزش می‌دهند و یاد می‌گیرند.\n",
    "\n",
    "# attention map consistency :\n",
    "\n",
    "برای جلوگیری از حفظ برچسب‌های نادرست توسط مدل، از ثبات نقشه توجه استفاده می‌شود. این تکنیک اطمینان می‌دهد که مدل روی ویژگی‌های مهم چهره تمرکز دارد، حتی زمانی که برچسب‌های شبه نویزی هستند.\n",
    "\n",
    "# cycling learning rate :\n",
    " یک نرخ یادگیری چرخه‌ای استفاده می‌شود تا از بیش‌برازش جلوگیری شود. این کار با شبیه‌سازی یک مجموعه از مدل‌ها در مراحل مختلف یادگیری انجام می‌شود.\n",
    "زیان طبقه‌بندی نمونه‌های حالت باز زیاد خواهد بود و این امر به مدل اجازه می‌دهد تا نمونه‌های حالت باز (با زیان بالا) را از نمونه‌های حالت بسته (با زیان کم) جدا کند."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

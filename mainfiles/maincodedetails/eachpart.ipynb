{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](/home/melika_yazdanpanah/fer/reaserches/Facial-Expression-Recognizer/PNG'S/codetoflow%20(7).png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the detailed sequence diagram using Mermaid syntax:The sequence diagram represents the interactions between the main function and the following functions:\n",
    "\n",
    "1. add_g(image_array, mean=0.0, var=30): This function adds Gaussian noise to the input image_array with the specified mean and variance.\n",
    "\n",
    "2. flip_image(image_array): This function flips the input image_array horizontally.\n",
    "\n",
    "3. setup_seed(seed): This function sets the seed for various random number generators to ensure reproducibility.\n",
    "\n",
    "4. generate_flip_grid(w, h, device): This function generates a grid of coordinates that can be used to flip attention maps.\n",
    "\n",
    "The diagram shows the flow of function calls, parameter passing, and return values between these functions. It provides a detailed understanding of the interactions and the internal logic within the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Here's the class diagram for the provided code using Mermaid syntax:Explanation:\n",
    "\n",
    "1. The ImageTransformer class is the main class that provides the image transformation functions.\n",
    "\n",
    "2. The add_g method adds Gaussian noise to the input image array.\n",
    "\n",
    "3. The flip_image method flips the input image array horizontally.\n",
    "\n",
    "4. The setup_seed method sets the seed for reproducibility across different libraries (PyTorch, NumPy, and random).\n",
    "\n",
    "5. The generate_flip_grid method generates a grid tensor that can be used to flip attention maps.\n",
    "\n",
    "6. The torch.Tensor class represents the PyTorch tensor data structure, with methods like to, unsqueeze, and expand.\n",
    "\n",
    "7. The numpy.ndarray class represents the NumPy array data structure, with methods like clip and astype.\n",
    "\n",
    "8. The torch.device class represents the PyTorch device (CPU or GPU) on which the tensors are stored.\n",
    "\n",
    "9. The relationships between the ImageTransformer class and the other classes are shown using association (--o) to indicate that the ImageTransformer class uses these classes as dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code includes several utility functions for image processing and data augmentation. Here's a breakdown of each function:\n",
    "\n",
    "1. add_g(image_array, mean=0.0, var=30):\n",
    "\n",
    "This function adds Gaussian noise to the input image_array.\n",
    "The noise is generated using a normal distribution with the specified mean and var (variance) parameters.\n",
    "The resulting image is clipped to the range [0, 255] and converted to uint8 data type.\n",
    "This function can be used to introduce noise or distortion to the input images, which can be useful for data augmentation.\n",
    "\n",
    "\n",
    "2. flip_image(image_array):\n",
    "\n",
    "This function flips the input image_array horizontally (left-to-right).\n",
    "It uses the cv2.flip() function from the OpenCV library to perform the flip operation.\n",
    "This function can be used to create a horizontally flipped version of the input image, which can also be useful for data augmentation.\n",
    "\n",
    "\n",
    "3. setup_seed(seed):\n",
    "\n",
    "This function sets the random seed for various libraries, including PyTorch, NumPy, and the built-in random module.\n",
    "It ensures that the random number generation is deterministic, which can be useful for reproducibility and debugging purposes.\n",
    "By setting the same seed value, you can ensure that the same sequence of random numbers is generated across different runs of your code.\n",
    "\n",
    "\n",
    "4. generate_flip_grid(w, h, device):\n",
    "\n",
    "This function generates a 2D grid of coordinates that can be used to flip attention maps or feature maps.\n",
    "The grid is created using PyTorch tensors and has the shape (1, 2, h, w), where the first dimension represents the batch size (set to 1), the second dimension represents the x and y coordinates, and the last two dimensions represent the height and width of the grid.\n",
    "The grid coordinates are normalized to the range [-1, 1], which is suitable for use with PyTorch's grid sampling functions, such as F.grid_sample().\n",
    "The grid is flipped horizontally by negating the x-coordinates, which can be useful for flipping attention maps or feature maps in a consistent manner with the flipped image.\n",
    "These functions can be used in various image processing and data augmentation tasks, such as:\n",
    "\n",
    "Adding Gaussian noise to input images for data augmentation.\n",
    "Flipping images horizontally for data augmentation.\n",
    "Ensuring reproducibility of random number generation across different runs of the code.\n",
    "Generating a grid of coordinates for flipping attention maps or feature maps in a consistent manner with the flipped image.\n",
    "The provided code can be integrated into your machine learning pipelines or image processing workflows to enhance the robustness and diversity of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code includes several utility functions for image processing and data augmentation. Here's a breakdown of each function:\n",
    "\n",
    "1. add_g(image_array, mean=0.0, var=30):\n",
    "\n",
    "This function adds Gaussian noise to the input image_array.\n",
    "The noise is generated using a normal distribution with the specified mean and var (variance) parameters.\n",
    "The resulting image is clipped to the range [0, 255] and converted to np.uint8 data type.\n",
    "2. flip_image(image_array):\n",
    "\n",
    "This function flips the input image_array horizontally (left-to-right).\n",
    "It uses the cv2.flip() function from the OpenCV library to perform the flip operation.\n",
    "3. setup_seed(seed):\n",
    "\n",
    "This function sets the random seed for various libraries, including PyTorch, NumPy, and Python's built-in random module.\n",
    "It ensures reproducibility of the random number generation, which is important for tasks like data augmentation and model training.\n",
    "4. generate_flip_grid(w, h, device):\n",
    "\n",
    "This function generates a 2D grid of coordinates for flipping attention maps.\n",
    "The grid is created using PyTorch tensors and has the shape (1, 2, h, w), where the first dimension represents the batch size (set to 1), the second dimension represents the x and y coordinates, and the last two dimensions represent the height and width of the grid.\n",
    "The grid coordinates are normalized to the range [-1, 1], and the x-coordinates are negated to achieve the horizontal flip.\n",
    "The grid is then moved to the specified device (e.g., CPU or GPU).\n",
    "These functions can be used in various image processing and data augmentation tasks, such as:\n",
    "\n",
    "Adding Gaussian noise to images for data augmentation during model training.\n",
    "Flipping images horizontally to increase the diversity of the training data.\n",
    "Generating a grid of coordinates for flipping attention maps, which can be useful in certain computer vision tasks.\n",
    "The setup_seed() function is particularly important when working with machine learning models, as it ensures that the random number generation is consistent across different runs, allowing for reproducible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code defines a ResNet architecture, which is a popular deep learning model used for image classification tasks. Let's break down the code and understand its functionality in detail.The provided code defines the following key components:\n",
    "\n",
    "1. Flatten: A PyTorch module that flattens the input tensor to have a batch size dimension and a flattened feature dimension.\n",
    "\n",
    "2. conv3x3: A helper function that creates a 2D convolutional layer with a 3x3 kernel size, specified input and output channels, and optional stride.\n",
    "\n",
    "3. BasicBlock: A basic building block for the ResNet architecture, consisting of two 3x3 convolutions, BatchNorm2d layers, and ReLU activations, with a residual connection.\n",
    "\n",
    "4. Bottleneck: A more complex building block for the ResNet architecture, consisting of three convolutions (1x1, 3x3, 1x1), BatchNorm2d layers, and ReLU activations, with a residual connection.\n",
    "\n",
    "5. ResNet: The main ResNet model, which consists of the following components:\n",
    "\n",
    "Initial 7x7 convolution, BatchNorm2d, and ReLU\n",
    "Max pooling layer\n",
    "Four ResNet layers, each created using the _make_layer function\n",
    "Final average pooling layer\n",
    "Flattening of the output tensor\n",
    "Final fully connected layer\n",
    "The _make_layer function is responsible for creating a ResNet layer, which consists of one or more BasicBlock or Bottleneck modules, depending on the block type and the number of blocks specified.\n",
    "\n",
    "The forward method of the ResNet class defines the forward pass of the model, where the input tensor is passed through the various layers and operations to produce the final output.\n",
    "\n",
    "The provided code also includes a main section that creates a ResNet-18 model (using BasicBlock and 2 blocks per layer) and a random input tensor, performs a forward pass, and prints the output tensor size.\n",
    "\n",
    "Overall, this code defines a powerful ResNet architecture that can be used for image classification tasks. The modular design and the use of PyTorch's built-in layers and modules make the code easy to understand and extend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the class diagram for the provided code using Mermaid syntax:Explanation:\n",
    "\n",
    "1. The Flatten class inherits from nn.Module and has a forward method that flattens the input tensor.\n",
    "\n",
    "2. The conv3x3 function creates a 3x3 convolutional layer with padding.\n",
    "\n",
    "3. The BasicBlock and Bottleneck classes inherit from nn.Module and represent the basic building blocks of the ResNet architecture.\n",
    "\n",
    "4. The ResNet class inherits from nn.Module and is the main class that defines the ResNet model. It has the following key components:\n",
    "\n",
    "conv1, bn1, relu, and maxpool layers for the initial processing of the input.\n",
    "layer1, layer2, layer3, and layer4 layers that use the BasicBlock or Bottleneck blocks to build the ResNet architecture.\n",
    "avgpool and fc layers for the final classification.\n",
    "The _make_layer method is used to create the ResNet layers.\n",
    "The forward method defines the forward pass of the ResNet model.\n",
    "The class diagram shows the relationships between the classes, including inheritance, composition, and aggregation. The cardinality and role names are also specified where applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss.py\n",
    "Purpose: Implements the Attention Consistency Loss (ACLoss) function.\n",
    "Function:\n",
    "ACLoss(att_map1, att_map2, grid_l, output): Computes the consistency loss between the attention maps of original and transformed images (flipped) using Mean Squared Error (MSE).\n",
    "Relation to Research: The attention consistency loss helps ensure that the model learns stable features even when images are transformed. This is particularly valuable for open-set recognition, where the model needs to remain robust against unseen variations in expressions.\n",
    "\n",
    "\n",
    "# close_set_training.py\n",
    "Purpose: Manages the training pipeline for closed-set facial expression recognition.\n",
    "Functionality:\n",
    "RafDataset Class: Loads and preprocesses the RAF dataset, implementing necessary data augmentations and creating a closed-set for training by excluding samples of the open class.\n",
    "Training Loop: Trains the ResNet model using cross-entropy loss to predict facial expressions from labeled images.\n",
    "Validation Loop: Evaluates model performance on a test set after each epoch.\n",
    "Relation to Research: This code establishes a baseline for the model's performance on closed-set recognition, allowing researchers to quantify how well the model can distinguish between known expressions. This is crucial before extending the model to open-set tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict_psedo_labels.ipynb\n",
    "\n",
    "![Alt Text](/home/melika_yazdanpanah/fer/reaserches/Facial-Expression-Recognizer/PNG'S/codetoflow%20(8).png)\n",
    "\n",
    "\n",
    "\n",
    "1. The code starts by importing the necessary libraries, including PyTorch, OpenCV, and various utility functions.\n",
    "\n",
    "2. The RafDataset class is defined, which is responsible for loading and preprocessing the RAF dataset for training and testing. It includes basic data augmentation techniques.\n",
    "\n",
    "3. The Flatten module is defined, which is used to flatten the input tensor before passing it to the final fully connected layer.\n",
    "\n",
    "4. The res18feature class is defined, which is a PyTorch module that extends the pre-trained ResNet-18 model. It loads the pre-trained weights and adds a final fully connected layer for classification.\n",
    "\n",
    "5. An argument parser is defined to handle various configuration parameters, such as the dataset path, pre-trained model path, and training hyperparameters.\n",
    "\n",
    "6. The seed is set, and the res18 model is created and loaded with the pre-trained weights.\n",
    "\n",
    "7. Data transformations are defined for both training and validation/test sets, including resizing, normalization, and conversion to PyTorch tensors.\n",
    "\n",
    "8. The training and test datasets are created using the RafDataset and TESTRAF classes, respectively, and the corresponding data loaders are initialized.\n",
    "\n",
    "9. The pre-trained weights for the res18 model are loaded.\n",
    "\n",
    "10. The TESTRAF class is defined to handle the test dataset, similar to the RafDataset class.\n",
    "\n",
    "11. The test data loader is created using the TESTRAF class.\n",
    "\n",
    "12. The model is evaluated on the test set, and the open-set and in-distribution scores are computed.\n",
    "\n",
    "13. The score distributions are visualized using a histogram plot.\n",
    "\n",
    "14. The FPR@95%TPR and AUROC metrics are computed to evaluate the model's performance on the open-set and in-distribution samples.\n",
    "\n",
    "15. The predicted labels for the open-set and in-distribution samples are saved to a file.\n",
    "\n",
    "The flowchart provides a comprehensive overview of the code's structure and the relationships between the different components. It highlights the key steps involved in loading the dataset, preprocessing the data, defining the model architecture, training and evaluating the model, and analyzing the open-set and in-distribution performance.\n",
    "\n",
    "Purpose: Predicts pseudo labels for open-set detection using the trained model.\n",
    "Key Steps:\n",
    "Loads the trained model and test dataset.\n",
    "Predicts expressions on the test set and stores these predictions.\n",
    "Analyzes the results and distinguishes between known and unknown expressions.\n",
    "Relation to Research: This notebook is essential for validating the model's effectiveness in open-set scenarios. It helps evaluate how well the model generalizes to unseen expressions, which is a significant challenge in FER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open_set_detection_set\n",
    "![Alt Text](/home/melika_yazdanpanah/fer/reaserches/Facial-Expression-Recognizer/PNG'S/codetoflow%20(9).png)\n",
    "\n",
    "\n",
    "1. Model Class:\n",
    "\n",
    "Loads a pretrained ResNet50 model\n",
    "Extracts features from the ResNet50 backbone\n",
    "Adds a fully connected layer for classification\n",
    "Computes attention maps using the extracted features and the fully connected layer weights\n",
    "2. Dataset Class:\n",
    "\n",
    "Loads the dataset from a CSV file\n",
    "Applies basic augmentation (e.g., flipping, adding Gaussian noise)\n",
    "Applies the necessary transforms (e.g., resizing, normalization)\n",
    "Handles clean and non-clean samples differently during training\n",
    "3. Training Function:\n",
    "\n",
    "Moves the model to the specified device\n",
    "Sets the model to training mode\n",
    "Computes the loss, backpropagates, and updates the weights\n",
    "Updates the learning rate scheduler\n",
    "Computes the training accuracy\n",
    "4. Testing Function:\n",
    "\n",
    "Moves the model to the specified device\n",
    "Sets the model to evaluation mode\n",
    "Computes the loss and accuracy on the test set\n",
    "5. Seed and Transforms:\n",
    "\n",
    "Sets a random seed for reproducibility\n",
    "Defines the training and evaluation transforms\n",
    "6. Dataset and Dataloader Creation:\n",
    "\n",
    "Creates the train and test datasets\n",
    "Creates the train and test dataloaders\n",
    "7. Model and Optimizer:\n",
    "\n",
    "Initializes the model\n",
    "Moves the model to the specified device\n",
    "Parallelizes the model\n",
    "Initializes the optimizer and scheduler\n",
    "8. Training Loop:\n",
    "\n",
    "Iterates over the specified number of epochs\n",
    "Trains the model using the training function\n",
    "Saves the trained model\n",
    "9. Evaluation:\n",
    "\n",
    "Evaluates the trained model on the test set\n",
    "Prints the test accuracy and loss\n",
    "The key aspects of this flowchart are:\n",
    "\n",
    "1. Modular Design: The flowchart is divided into several subgraphs, each representing a specific component or functionality of the system. This modular design makes the code more maintainable, testable, and easier to understand.\n",
    "\n",
    "2. Detailed Explanations: Each component of the flowchart is accompanied by a detailed explanation of its purpose, functionality, and the relationships between the different elements.\n",
    "\n",
    "3. Comprehensive Representation: The flowchart covers the entire process of training the open-set detection model, from data loading and preprocessing to model training, evaluation, and saving the trained model.\n",
    "\n",
    "4. Attention Map Computation: The key feature of the model is the computation of attention maps, which are used to enhance the model's performance in open-set detection tasks.\n",
    "\n",
    "5. Handling Clean and Non-Clean Samples: The dataset class is designed to handle clean and non-clean samples differently, which is an important aspect of the open-set detection problem.\n",
    "\n",
    "6. Optimization and Scheduling: The training function includes the optimization process, where the weights are updated, and the learning rate scheduler is used to adjust the learning rate during training.\n",
    "\n",
    "7. Evaluation and Reporting: The flowchart includes the evaluation of the trained model on the test set and the reporting of the test accuracy and loss.\n",
    "\n",
    "Overall, this detailed flowchart provides a comprehensive understanding of the open-set detection model training process, including the key components, their interactions, and the underlying technical details."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
